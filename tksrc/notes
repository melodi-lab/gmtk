Various notes to myself (JB) on different algorithms and other
features of GMTK.

J. Bilmes, $Header$

======================================================================
======================================================================
======================================================================

Algorithm for splitting and vanishing Gaussians with sharing.

   If a Gaussian component is split, it gets cloned
   meaning that its mean and covariances are cloned.
   If another Gaussian component is split, and it shares
     either the mean/variance of another one that was split,
     it will continue to share the mean/variance of the cloned
     Gaussian.
   The cloning relationships are forgotten once all is done at the 
   end of the epoch.

Need three maps here, cleared out at the end of each epoch.
   remove map
    DPMF ptr & unsiged -> dummy;
   added map
    DPMF ptr & unsigned -> dummy
   addd map 
    DPMF ptr & cmp ptr -> cmp ptr

Need more maps for cloning, these are cleared out at the end of each epoch.
They live in GM_Params.
   map from mean -> its clone
   map from covar -> its clone
   map from lincondvar -> its clone
   map from GC -> its clone   

When we clone a GC
   if GC has clone in map, use that
   else
       clone mean
       clone var

clone mean
   if mean has clone in map use that,
   else really clone

clone covar
   analogous to clone mean
       
The reason for using maps here rather than additional members in
each object is that splitting/vanishing is probably the exception
rather than the norm, therefore we don't want to use up
more memory by adding a field in each object which will
probably not often be used.


mix gaus end 

   end the PMF
   to through pmf,  entry i
         end comp epoch
              // need to end comp epoch in any case because
              // otherwise cov ref count won't hit zero.
         if pmf i is below threshold
               add DPMF ptr & i key to set of removed things
         if pmf i is above threshold
               add DPMF ptr & i key to set of added things

mixgauss swap

   check on DPMF's new length

   go through, entry i
       if pmf&i is in delete set
           skip it, don't swap, move to last position
       if pmf&i ptr in add set
           // can't just use cmp ptr here since it might be shared &
           // have been cloned for one MG but not for another.
           clone cmp i and add it in.


densePMF swap

   newLen = len;
   go through pmf, entry i
      if i is in delete set
         newLen --;
      if i is in clone set
         newLen ++
   make new len array
   go through pmf, entry i
      if i is in delete set
         skip j
      if i is in clone set
         add i an j with split prob
      else
         just skip
   normalize


OLD ========================================


mixgauss swap

   go through, entry i
       if pmf&i is in delete set
           skip it, don't swap
       if pmf& cmp ptr in add set
           // can't just use cmp ptr here since it might be shared &
           // have been cloned for one MG but not for another.
           add cmp & its clone to new thing      
           if DPMF ptr & cmp ptr not in that map
                 clone and perturb entry i
                  add DPMF & cmp ptrs to that map


===============================

======================================================================
======================================================================
======================================================================


to get compiling on IBM

 -D__GNUC__=2 -D__GNUG__=2 -D__GNUC_MINOR__=95 -D__cplusplus -trigraphs -D__STRICT_ANSI__ -D_IBMR2 -D_POWER -D_AIX -D_AIX32 -D_AIX41 -D_AIX43 -D_LONG_LONG -Asystem(unix) -Asystem(aix) -D__EXCEPTIONS -D__CHAR_UNSIGNED__ -g -Wall -pedantic -D_ARCH_COM

- GMTK_FileParser.lex
  already defined fileno

- no libsunmath to use.

- using g++ w/o a path in makefiles

- ieeefp.h

- fileParser.cc
     write() overload problem.

- ieeefp from pfile




======================================================================
======================================================================
======================================================================

Originally file 'data_items_needed,v 1.7 2001/05/26 03:53:40 bilmes Exp'

Statistical Data Types Needed

(not the data structures, just what is kept in the files)


----------------
Dense 1D Distributions: GMTK_Dense1DPMF
 : 1D arrays that sum to 1 and are >= 0
  Needed for:
     1) mixture models
     2) 

----------------
Sparse 1D Distributions: GMTK_Sparse1DPMF
  : 1D arrays of <int value, probability> pairs,
   used for distributions with lots of zeros.

----------------
1D array of reals: GMTK_RealArray
  : linear dependency links
  Needed for:
      means
      diagonal covariances

----------------
Mean arrays: GMTK_MeanVector
	uses GMTK_RealArray

----------------
Diagonal Covariances: GMTK_DiagCovarVector
	uses GMTK_RealArray

----------------
2D array of reasls: GMTK_RealMatrix.h
  : needed for MLP weight matrices
  : needed for log. regression "weight" matrices

----------------
2D packed-sparse array of reals:  GMTK_PackedSparseRealMatrix.h
   : needed for blink values.

----------------

Dlink values: GMTK_DlinkMatrix
   : the actual values used for linear cross-obs. dependencies

----------------
Full Dense N-D Distributions: 
  . There are several different types:
   1) GMTK_MDCPT
       Multi-d Dense CPT
   2) GMTK_MSCPT
       sparse, decision tree based CPT. 
       sparse in two senses
          a) parents state space are divided into regions of 
             same child probabilities.
          b) child probabilities are sparse also (zeros aren't stored)
   : used for things such as
       - markov chain stochastic matris
       - parameters for a variable along with 
         conditional parents

----------------
OBservation DLINKS: GMTK_DLINKS
  : 2D arrays of 2-tuples containing <time lag, feature offset>
  The time lag says where, relative to the current position
  the feature dependency is, and the feature offset
  says to which feature relative to feature 0.
  Rows are for each feature, columns are the dlink tuples.
  Needed for:
      1) conditional gaussians
      2) all other observation conditioned obs variables.

N
0
0 3 -1 4 -2 5 -5 3
1 0
2 1 -4 6
3 4 -4 5 -1 5 -10 14 -2 3
1
0 3 -1 4 -2 5 -5 3
1 0
2 1 -4 6
3 4 -4 5 -1 5 -10 14 -2 3
2
0 3 -1 4 -2 5 -5 3
1 0
2 1 -4 6
3 4 -4 5 -1 5 -10 14 -2 3



============================================================
Basic Gaussian Components.

----------------
Diagonal Covariance Gaussians, GMTK_DiagGaussian  
  : uses 
      1) array of reals for means
      2) array of reals for variances

Gaussian looks like:


<gaussian number> <index into mean vector> <index into variance vector>

E.g.,

N # number of Gaussians to follow.
0 name0 5 10
1 name1 6 10
2 name2 7  3
...



Mean Conditional Gaussian, GMTK_LinMeanCondDiagGaussian
Diagonal Covariance Gaussians with linear mean dependency links
  : uses 
      1) array of reals for means
      2) array of reals for variances
      3) 2D array of dlinks
      4) 2D array of reals for linear conditioning values

Looks like:

<number> <mean index vector> <variance index vector> <dlink index> <2D Array index>

E.g.,
N
0  5 10  3  4
1  0  4  3  1
2 10  3  2  5
3 11 23  4  9


Diagonal Covariance Gaussians with linear & non-linear mean dependency links,
GMTK_NLinMeanCondDiagGaussian
  : uses 
      1) array of reals for means
      2) array of reals for variances
      3) 2D array of dlinks
      4) 1D array saying how many for each feature is linear and
	 how many is non-linear
      5) 2D array of reals for linear conditioning values
      6) 2D array of reals for linear conditioning values    


<gaussian number> <gaussian type integer> <variable number of additional tags>

<gaussian type integer> could be
   0 := vanilla diagonal covariance Gaussian
     Additional tags become:
        <mean index vector> <variance index vector>
   1 := Linear Mean Conditional Gaussian, only linear dependencies
     Additional tags become:
       <mean index vector> <variance index vector> <dlink index> <2D packed sparse Array index>
   2 := Diagonal Covariance Gaussians with lin + non-lin mean dep. links.
     Additional tags become: 
       <mean index vector> <variance index vector> <dlink index> <2D packed sparse Array index> <dlink index> <2D dense array index> <2D dense array index>
     The first dlink index and packed sparse array index is for the
       linear part. The index may be -1 to indicate that this doesn't exist.
       The second dlink index and 2 matrix indices are for the non-linear
       portion where the two matrices are for the MLP weight matrices. 
       Note that the dimensions of the dense array must match, or an error
       will occur.
      (might need a list of MLPs here, one for each feature element).

Examples:

E.g.,

N # number of Gaussians to follow.
0 0 5 10  # plain vanilla diag cov Gaussians
1 0 6 10
2 0 7  3
3 1 23 3 5 10 # linear conditional mean Gaussians
4 1 7  3 6 10 
5 2 23 3 5 4 5 10 12 # linear/non-linear conditional mean Gaussiansa
6 2 7  3 5 4 5 10 12


<Number> <Dimension> <Type> <Typedep stuff>

  <typedep stuff> 
        GaussianCompName MeanName CovarianceName


N # number of Gaussians to follow.
0 0 5 10  # plain vanilla diag cov Gaussians
1 0 6 10
2 0 7  3
3 1 23 3 5 10 # linear conditional mean Gaussians
4 1 7  3 6 10 
5 2 23 3 5 4 5 10 12 # linear/non-linear conditional mean Gaussiansa
6 2 7  3 5 4 5 10 12


============================================================================
============================================================================
============================================================================

The below are the 'only' things that may be true observation densities.
All of the above


Mixture Gaussians, 
GMTK_MixGaussians
  (note to have a single Gaussian, we need a mixture of 1 Gaussian component)
  : uses
     1) 1D prob. distribution
     2) list of some number of above Gaussians (of various types)

N
<index number> <number of mixtures> <index to 1d dist> 
           <list of integers pairs (GausType Number) indexing to gaussian>

0 5 3 0 1 0 2 0 5 0 6 0 7 # all Diag. Cov. Gaussians
1 5 3 1 1 1 2 1 5 1 6 1 7 # all Linear Mean Cond. Gaussians
2 5 3 2 1 2 2 2 5 2 6 2 7 # all Non-Linear Mean Cond. Gaussians
3 5 3 0 1 1 2 2 5 0 6 1 7 # mixture of heterogeneous Gaussians.
4
5
...




Switching Gaussian Mixture with Gaussian Classifier Switching, 
GMTK_GausSwitchingMixGaussians
  : uses
    a) for the switching distribution
       1) 1D prob distribution over switch variables
       2) array of Gaussians
             constraint on the Gaussians is that the dlink
	     variables (if any) must have only zero-lag links.
      Note that the dim of the Gaussians must be equal to the union 
        of all possible dependency variables from part b.
    b) for the collection of Gaussian mixtures
       1) array of Gaussian mixtures (of some type, possibly
	  heterogeneous)



N
<index number> <number of mixtures> <index to 1d dist> <list of integers indexing to one of the gaussians>

Note: these Gaussians will need to be different than the ones above in
that they take a sparse array of features (this can be done using
the existing Gaussian code by allowing for the feature vectors to be
sparse collections of features (by giving one of the 2d arrays of Dlinks
for the x vector).

Switching Gaussian Mixture with Logistic Regression/classification Switching
GMTK_LogitSwitchingMixGaussians
  : uses
    a) for the switching distribution
       1) 2D array of reals for the regression.
      num Rows = switching size
      num cols must be equal to the union 
              of all possible dependency variables from part b.
    b) for the collection of Gaussian mixtures
       1) array of Gaussian mixtures (of some type, possibly
	  heterogeneous)


N
<index number> <number of mixtures> <index to 1d dist> <index of dense array>

(the dense array is the weight matrix for the logistic regression).
Note that the dimensions of the array must match, or an error
message will occur.

Switching Gaussian Mixture with 3layer Neural Network switching
GMTK_MLPSwitchingMixGaussians
  : uses
    a) for the switching distribution
       1) 2D array of reals for input to hidden
	      num Rows = number of NN hidden units
	      num cols must be equal to the union 
                  of all possible dependency variables from part b.	
       2) 2D array of reals for hidden to output (assume logistic)
	      num Rows = switching size
	      num cols = num of NN hidden units.
    b) for the collection of Gaussian mixtures
       1) array of Gaussian mixtures (of some type, possibly
	  heterogeneous)



N
<index number> <number of mixtures> <index to 1d dist> <index of dense array>
<index of dense array>

(the dense arrays are the weight matrices for the non-linear regression).
Note that the dimensions of the array must match, or an error
message will occur.


============================================================================
============================================================================
============================================================================

Questions:

   1) how to do the group stuff, G
       In a frame, we specify observation vector variables.
       The vector has a discrete set of parents which, by some mapping
       function (e.g., decision tree), map to an integer which indices 
       to the appropriate observation type. The observation vector also 
       associates itself with a range of features. The mapping can 
       be learned, and this is how we implement "state tying".

       Each frame can have several scoring observation vectors,
       the score for the final utterance will be the product of
       all scoring observation probabilities. 
    

   2) how to unify this description with the feature description
      from before.

   3) observed discrete at training, hidden discrete at testing.

   4) discrete observations.

   5) Arrays of discrete and continuous features:
        1) need to ensure Gaussian doesn't index into discrete Gaussian.


Syntax for specifying mapping from switching parent values
to integer values.

RV has a number of switching parents.

File syntax:
Switching parents are S1, S2, ..., Sn

SwitchingParents: S1, S2, ..., Sn
ConditionalParents:
  0:5   0:9   0:5 : C1 C2 ... Cn
  6:10  11:12 6:9 : 
  ...
  default: C1 C2 ... Cm


Decision trees on vectors of integers.

<numFtrs>
<ftr> <num_splits> r1 r2 ... rs 
   <ftr> <num_splits> r1 r2 ... rs # split 1
   <ftr> <num_splits> r1 r2 ... rs # split 2
   -1 value
   <ftr> <num_splits> r1 r2 ... rs # split s
 

0 r0
  1 r1


6 types of files:

- Master file
   has 
    o command line options
    o other file names
    o various switches, etc.
- structure file
    o specifies the basic structure of the
      hidden variables and between hidden and observations.
- intra observation structure file
   o list of DLINK matrices,
     specifies 
- DT file
   o gives a large list of decision trees.
- observation distribution file
   o lists of Gaussian mixtures and other such things.
     Basically a list of integer pointers.
- generic data file
   o generic data that Gaussian mixtures and other files
     might use such as:
     - 1d dists
     - vectors
     - means
     - variacnes
     - matrices
     - single component gaussians
     - etc.



======================================================================
======================================================================
======================================================================


===============================

notes on new tying.

1) add a shared bit to EMAble to say that these parameters
   are shared. Not necessarily used by all EMable objects,
   but the bit value is defined here anyway since it
   is just another bit.


2) mean vector emEndIteration
    if no sharing is taking place, do the normal thing,
    otherwise, use the pi and C matrices to normalize
    the incoming partially accumulated means.


 

===============================

notes on face algorithm, all done after
moralization on an undirected graph.

start off with template, unrolled 0 times
  of already moralized (but not yet triangulated) graph

create sets P, C, and E containing
       variables in prologue, chunk, and epilogue

create interface sets
   C_l = left interface,
       = all nodes of C that have a neighbor in P
   C_r = right interface
       = all nodes of C that have a neighbor in E

Goal is to find a change of C that results
   in the minimum of min(|C_l|,|C_r|)

Note: not allowed to have a node that goes
      from any node in P directly to E (i.e.,
      C separates P from E in the undirected graph)


Left-interface only exhaustive search algorithm.

set such that current P,C_l is best at the moment

find_best_left_interface(C_l)

  for all nodes n in C_l, do
     // rather than "for all nodes in C_l", we could
     // do a random subset of nodes in C_l to speed this up if
     // it takes too long. But note that this is only run once
     // per graph so it will be beneficial to do this since
     // its cost is ammortized over the many runs of the graph


     if n has neighbors in E
         next;
     // so n has neighbors in P (since it is in C_l) but not in E


     // add n from C_l and place it in P
     // and add all neighbors of n that are in C\P' to C_l
     P' = P + n;
     C'_l = C_l - n + (neighbors of n that are in C\P')
	
     if hash_hit(C'_l)
         next;

     store the candidate P' and C'_l, if its size (or weight) is min so far,
          and so on as a possible 

     store C'_l in hash for memoization.

     find_best_left_interface(C'_l)

Right-interface algorithm is the same, but inverted.



General (left interface) algorithm to find the chunks of graph to triangulate.
 - valid only when number of frames is such that graph can
   be unrolled at least one time.

 - start with P, C1, C2, C3, and E (graph unrolled 2 times)
    and find best left interface within C2
    gets best C_l and best left_C_l
    (i.e., call them C_l and left_C_l)

 - form P',C1',C2',E' (i.e., graph unrolled 1 time)
 - make a mapping from nodes in 
    1) C2 to corresonding nodes in C1'
    1) C2 to corresonding nodes in C2'

 - make_complete C1'(C_l)
 - make_complete C2'(C_l)  

 - form sets
    A: P' + C1'(left_C_l) + C1'(C_l)
    B: C1'\C1'(left_C_l) + C2'(left_C_l) + C2'(C_l)
    C: C2'\C2'(left_C_l) + E

 - clone sets A, B, and C
 - triangulate separately A, B, and C
 - form cliques of A, B, and C
 - joint cliques together.



 - form associate mapping C^1(vars) : C^2 -> C^1 
                    and   C^2(vars) : C^1 -> C^2    
                   (possibly also do something with P and E)
   to map from corresponding sets of vars in either 
 - find best left interface in P and C^1 -> P', C'_l
   (the output)
 - make C'_l into a complete set
 - make C^2(C'_l) into a complete set
 - form set union (P',C'_l)
 - Pbar = union of (P',C'_l)
 - Cbar = C^1\P' U C^2(P'\P U C'_l)
   (which necessarily contains C'_l)
 - Ebar = E U C^2\Cbar
 - clone Pbar, Cbar, Ebar into P'bar, C'bar, E'bar
 - triangulate P'bar, C'bar, E'bar
     and form cliques of graphs.


---------------------------------


n[] is array of nodes in topological order
     (meaning a variable never appears before
      all its parents have already appeared.
      Variables w/o parents can occur at any time)
cl[] is clique array haivng
   cl[i].member (nodes that are in the current clique)
   cl[i].newMember (nodes in cl[i] that are not in 
                    previous max clique)
                  = cl[i].member \ cl[j].member
                  where j is index of previous maxclique.
         This is used for conditional probability nodes
	 during inference, and can be ignored just
         when understanding the triangulation algorithm.

Alg:
add n[0] to cl[0].member
add n[0] to cl[0].newMember
for i = 1 .. num_nodes
   // at this point cl[i-1] is the current candidate maxclique

   // start off by duplicating previous cliques new members
   cl[i].newMember = cl[i-1].newMember

   // The next loop does a deletion. Note that if no deletions
   // occurs at all in the loop, then the loop just copies over previous
   // cliques members to new clique members,
   // i.e., cl[i].member = cl[i-1].member
   // (but this occurs by taking the else condition each time)
   for m in all of cl[i-1]'s members
      if all m's children are in current frontier clique (cl[i-1])
          // see *** below
      then 
           // Do a "deletion", which means we do not add m to the
	   // new clique cl[i].member.
           // This makes cl[i-1].member maximal. 
           // cl[i-1].member is maximal because:
           //  1) it is certainly not a subset of any future maxclique
           //     since no future clique could possibly contain m
           //  2) it is not a subset of any past (i-2, i-3, ...) maxclique 
           //     because n[i-1] was added only to cl[i-1].member at the
           //     previous i iteration. I.e.,
           //     n[i-1] was not added to any earlier maxclique.
	   //
           // Note that because we are producing a "deletion"
           // by searching for any parents who are such that
           // all their children are in the current frontier clique
	   // cl[i-1].members, it is possible that the "deletion" might 
           // happen multiple times for diff m. But in each case the same 
           // maximal clique cl[i-1].members will arise. 
           // Because maximal_set it is a set, it doesn't matter
           // if cl[i-1].members is inserted multiple times.
           insert previous clique cl[i-1] into maximal_set
           // Next, we clear out newMember of current clique since
           // at this point no new members have been added that would
           // not be in previous max clique which is now cl[i-1].member.
           clear out cl[i].newMember
      else     
           add m to cl[i].member
   add n[i] to cl[i].member
   add n[i] to cl[i].newMember
end_for

insert last clique cl[i].member into maximal set


***
Refering to the line
   "if all m's children are in current frontier clique (cl[i-1])"
the standard algorithm says that all of m's children
are in the current frontier clique, then m can be deleted. What
is actually implemented in GMTK however is a check to
see if A == B, where A and B are defined as:

   A = the number of children of m that has been
       added to a frontier clique (at one time or another) in the past
   B = the total number of children of m

If A == B, then m may be removed from the frontier clique. This is
different than all of m's children being in the current frontier
clique as that might lead to a condition where a child (say CH) of a
node (say N) might come an go so that all of N's children are never at
the same time in the frontier clique. This could happen if for example
CH is placed in the frontier clique, more nodes are added, all of CH's
children get inserted, and then CH is removed, but this happens before
the rest of N's children are in the clique. It seems like this other
condtion (A==B) is necessary to ensure that this condition doesn't
occur, as otherwise N will never be removed.



---------------------------------

Sun Dec 22 20:38:15 2002
- Unrolling currently allows for links accross the chunk boundary.


idea:
   create a routine to clone a collection/set of random variables
   (i.e., keeps the same structure topology amongst the 
    new rvs.)

   - clone set of rvs
   - makeComplete (make a set of random varibles complete)
   -     





Thu Jul 03 19:08:30 2003

GMTK_GMTemplate
contains:
  - paritions
  - cliques
  - reading/writing routines for partitions/cliques
  - triangulation by clique completion
  - partition creation routines from a PartitionStructures

BoundaryFindingStructures:
  - data structures stuff that gets searched to find boundary

PartitionStructures:
  - stuff that gets partitioned to create gm partitions.
    but this is re-copied anyway since we need to form the
    partially disjoint thing.

Boundary algorithm code (which needs to call triangulate to evaluate partition)
  - 

Code that forms partitions from current boundary
  - findInterfacePartitions
  - creates a GMTempalte with partitions but w/o cliques
    (code for this is in GMtemplate, uses
       setUpClonedPartitionGraph(loc_P,loc_C,loc_E,P,C,E,P_in_to_out,C_in_to_out,E_in_to_out);
    )

Triangulating code
  - need to be able to untriangulate since it tries multiple times.
  - should operate on individual partitions within a GMTemplate
  - does not operate directly on a GMTempalte
  - can have a separate triangulation routine that operates on a GMTemplate,
    or GMTemplate can have a triangulate routine even. May or may not be useful.

    
